{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning \n",
    "\n",
    "__Suppervised learning__\n",
    " * Classification\n",
    " * Regression\n",
    "\n",
    "__Reinforcement learning__ \n",
    " * Exemple : AI de jeu-video, Navigation Tesla, Decisions temps-réel.\n",
    "\n",
    "__Unsupervised learning__ \n",
    " * Reduction (Dimensionally reduction)\n",
    " * Regroupement (Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification vs Regression \n",
    "\n",
    "__Classification__\n",
    " * Prédire une catégorie. \n",
    "\n",
    "__Regression__\n",
    " * Prédire une valeur numérique (Quantitative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces problèmes peuvent etre représenter de la manière suivante :\n",
    "\n",
    "$$Y = f(X) + \\epsilon$$\n",
    "\n",
    "| | Signification |\n",
    "|-----|----------------------------|\n",
    "| $Y$ | la variable Cible (target) |\n",
    "| $X$ | les valeurs Explicatives : |\n",
    "|  | $n$ : lignes|\n",
    "|  | $p$ : colonnes |\n",
    "| $f$ | fonction |\n",
    "| $\\epsilon$ | le bruit (Episilon) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valeurs catégorique et numérique \n",
    "\n",
    "| Valeurs |  | Signification |\n",
    "|--------------------------|-----------|--------------|\n",
    "| Quantitative (Numérique) | Continue | Float |\n",
    "|                          | Discrète | Entier ou notion de pas |\n",
    "| Qualitative (Catégoriel) | Ordinales | Note A, B, C |\n",
    "|                          | Nominales | Sexe F ou M |\n",
    "\n",
    "\n",
    "$X$ est une valeur explicative.\n",
    "\n",
    "$Y$ est une valeur cible (target). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préprocesing : Netoyage \n",
    "__Supression de colonnes__\n",
    " * Valeurs uniques : ID, Nom, Siret...\n",
    " * Trop tronquées : Plus de 60 % de valeur manquante. \n",
    " * Valeurs Nominales excessives : 36 categories.\n",
    " * Une des deux valeurs colineaires : Male 0 . 1 & Femal 1 . 0.\n",
    "\n",
    " __Supression de lignes__\n",
    "  * Valeurs cible manquante (Y).\n",
    "  * Les valeur aberrante (Outlier).\n",
    "\n",
    "### Complétions des valeurs manquantes \n",
    "__Valeurs Quantitative__\n",
    " * Moyenne(mean) : parfois sur les valeurs continues.\n",
    " * Mediane(median) : à utiliser en priorité.  \n",
    "\n",
    "__Valeurs Qualitatives__\n",
    " * valeur la plus fréquente (mode).\n",
    " * remplacer par un mot clef comme Missing Value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation \n",
    "__Normalisation :__\n",
    "- Notre valeur $X$ est remplacé par la valeur de cette formule : \n",
    "\n",
    "$$\\frac{X - \\bar{X}}{\\sqrt(Var(X))}$$\n",
    "\n",
    "|                 | Signification |\n",
    "|-----------------|-----------------------------------------|\n",
    "| $X - \\bar{X}$   | Supression de la moyenne de la variable |\n",
    "| ––––––––––––––– | divisé par |\n",
    "| $\\sqrt(Var(X))$ | L'écart type de la variable |\n",
    "\n",
    "Ce qui entraîne une variable de moyenne 0 et un écart type (standard deviation) de 1.\n",
    "\n",
    "\n",
    "_A noter que la normalisation se fait sur les valeurs explicatives ($X$) mais jamais sur la cible ($Y$)_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage \n",
    "__One Hot Encoding (or Dummy Encoding)__\n",
    "\n",
    "Les valeurs qualitatives (catégoriel) sont ecrasé en colonnes binarisé. \n",
    "\n",
    "| Pays    |\n",
    "|---------|\n",
    "| France  |\n",
    "| Espagne |\n",
    "| Italie  |\n",
    "\n",
    "Devient : \n",
    "\n",
    "| France | Espagne | Italie |\n",
    "|--------|---------|--------|\n",
    "|1|0|0|\n",
    "|0|1|0|\n",
    "|0|0|1|\n",
    "\n",
    "Puis décolinéarisé :\n",
    "\n",
    "| Espagne | Italie |\n",
    "|---------|--------|\n",
    "|0|0|\n",
    "|1|0|\n",
    "|0|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régressions linéaires (Régression)\n",
    "$$\n",
    "Y = \\beta_{0} + \\beta_{1}X_1 + \\epsilon\n",
    "$$\n",
    "\n",
    "|             | Signification |\n",
    "|-------------|----------------------------|\n",
    "| $Y$         | la variable Cible (target) |\n",
    "| $X_1$       | la variable Explicatives |\n",
    "| $\\beta_{0}$ | Constante, là ou commence la droite à $X_1 = 0$ |\n",
    "| $\\beta_{1}$ | Coefficient directeur (pente / slope) |\n",
    "| $\\epsilon$  | le bruit (Episilon) |\n",
    "\n",
    "$Y =  ax + b + \\epsilon$ (erreur)\n",
    "\n",
    "Pour que la regression lineaire soit vrai :\n",
    "\n",
    "* __Linearity :__ les points doivent suivre une droite.\n",
    "* __Homoscedasticity :__ l'erreur $\\epsilon$ doit etre constante et ne pas suivre l'echelle de $X_1$.\n",
    "* __Independence of residuals :__ les erreurs ne doivent pas etre corrélés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation : Méthode des moindres carrés ordinaire\n",
    "_ordinary least square method est une fonction de perte_\n",
    "\n",
    "![](https://user.oc-static.com/upload/2018/11/14/15421935105892_P2C2_Graphique1.jpg)\n",
    "\n",
    "$$\n",
    "Min (\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2)\n",
    "$$\n",
    "\n",
    "En décomposant $\\hat{y}$, on obtient :\n",
    "\n",
    "$$\n",
    "Min (\\sum_{i=1}^{n}(y_{i}-\\beta_{0}-\\beta_{1}x_{i})^2)\n",
    "$$\n",
    "\n",
    "|               | Signification |\n",
    "|---------------|----------------------------|\n",
    "| $y_{i}$       | position reel du point |\n",
    "| $\\hat{y}_{i}$ | estimation de point de la regression lineaire |\n",
    "|               | $ax_{i} + b$ |\n",
    "| $\\sum$    | somme |\n",
    "| $n$ | ensemble n |\n",
    "| $i$ | debut des iterations |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régressions linéaires multiple \n",
    "\n",
    "Une première étape est de __normaliser__ les resultats pour leur donner une valeur moyenne de 0 et un écart type de 1, ainsi les valeurs explicatives sont comparable. \n",
    "\n",
    "Cette formule correspond à la précedente pour de multiple valeurs explicatives :\n",
    "\n",
    "$$\n",
    "Y_{i} = \\beta_{0}+X_{i,1}\\beta_{1}+...+X_{i,p}\\beta_{p}+\\epsilon_{i}\\forall i \\in \\left [1,n \\right ]\n",
    "$$\n",
    "\n",
    "Qui peut etre résumé sous forme matriciel : \n",
    "\n",
    "$$\n",
    "Y=X \\times \\beta+\\epsilon\n",
    "$$\n",
    "\n",
    "|               | Signification |\n",
    "|---------------|----------------------------|\n",
    "| $Y$ | Vecteur de dimention $(n, 1)$ |\n",
    "| $X$ | Matrice de dimention $(n, p + 1)$ |\n",
    "| $\\beta$ | Vecteur de dimention $(p + 1, 1)$ |\n",
    "| $\\epsilon$ | Vecteur de dimention $(n, 1)$ |\n",
    "\n",
    "\n",
    "_1 : Il faut impérativement éviter les variables explicatives colinéaire_\n",
    "\n",
    "_2 : Il faut supprimer les valeurs aberante pour ne pas biaiser l'erreur ($\\epsilon$)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation de modèles\n",
    "\n",
    "### Analyse de la variance (ANOVA)\n",
    "\n",
    "![](https://i.stack.imgur.com/FOzPq.png)\n",
    "\n",
    "__SST : Sum of Square Total :__ \n",
    "- Indicateur de dispersion des valeurs de $Y$ (variable cible ou target) \n",
    "\n",
    "$$\n",
    "SST = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^2\n",
    "$$\n",
    "\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|----------------------------|\n",
    "| $y_{i}$ | une des variable cible |\n",
    "| $\\bar{y}$ | moyenne des variable cible |\n",
    "| $\\sum$ | somme |\n",
    "| $n$ | ensemble n |\n",
    "| $i=1$ | debut des iterations |\n",
    "\n",
    "_C'est la différence au carré entre la variable dépendante observée et sa moyenne._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__SSR : Sum of Squares due to Regression :__ \n",
    "- une mesure qui décrit dans quelle mesure notre prediction correspond aux données.\n",
    "\n",
    "$$\n",
    "SSR =\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2\n",
    "$$\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|----------------------------|\n",
    "| $y_{i}$ | une des variable cible |\n",
    "| $\\hat{y}_{i}$ | estimation de point de la regression lineaire |\n",
    "| $\\sum$ | somme |\n",
    "| $n$ | ensemble n |\n",
    "| $i=1$ | debut des iterations |\n",
    "\n",
    "\n",
    "_C'est la somme des différences au carré entre la valeur prédite et la moyenne de la variable dépendante._\n",
    "\n",
    "_Entre autre obtenir le plus petit SSR possible revient à appliquer la méthode des moindres carrés._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__SSE : Sum of Squares Error :__\n",
    "- une mesure qui représente la quantité de dispersion de la variable cible.\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=1}^{n}(\\hat{y}_{i}-\\bar{y})^2\n",
    "$$\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|----------------------------|\n",
    "| $\\hat{y}_{i}$ | une des variable cible predicte |\n",
    "| $\\bar{y}$ | moyenne des variable cible dépendante |\n",
    "| $\\sum$ | somme |\n",
    "| $n$ | ensemble n |\n",
    "| $i=1$ | debut des iterations |\n",
    "\n",
    "_C'est la somme des différences moyennes au carré entre les estimations du modèle pour chaque observation et la moyenne de la variable cible pour la population d'intérêt._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Resumé__\n",
    "* SST est la quantité totale d'informations.\n",
    "* SSE est l'information expliquée par le modèle.\n",
    "* SSR est l'information qui reste à expliquer, ou l'erreur commise.\n",
    "\n",
    "$$SST = SSR + SSE$$\n",
    "\n",
    "ou \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n}(y_{i}-\\bar{y})^2 = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2 + \\sum_{i=1}^{n}(\\hat{y}_{i}-\\bar{y})^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__R2 : R-carré :__\n",
    "- une statistique qui quantifie le pouvoir explicatif du modèle par rapport à la variable cible.\n",
    "si le modèle permet d'expliquer fidèlement la variable cible, alorsSSRsera plus proche de 0 etR2sera plus proche de 1.\n",
    "\n",
    "\n",
    "$$\n",
    "R^2 = 1-\\frac{SSR}{SST}\n",
    "$$\n",
    "\n",
    "ou\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{SSE}{SST}\n",
    "$$\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|------------------------------|\n",
    "| $SSE$ | Sum of Squares Error |\n",
    "| $SSR$ | Sum of Squares due to Regression |\n",
    "| $SST$ | Sum of Square Total |\n",
    "\n",
    "\n",
    "_C'est la somme des différences moyennes au carré entre les estimations du modèle pour chaque observation et la moyenne de la variable cible pour la population d'intérêt._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression\n",
    "\n",
    "* $n$ le nombre de d'observation/échantillons.\n",
    "* $p$ le nombre de variables explicatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonction de coût \n",
    "\n",
    "En Supervised Learning, la machine cherche les paramètres de modèle qui minimisent la Fonction Coût.\n",
    "\n",
    "La fonction de coût pénalise la distance d’un point où la prédiction est mauvaise à la frontière entre deux classes.\n",
    "\n",
    "La fonction de coût (cost function):\n",
    "\n",
    "$$\n",
    "||y-X^{t}\\beta||^{2}_{2}\n",
    "$$\n",
    "\n",
    "L'erreur quadratique moyenne (Mean Square Error): \n",
    "\n",
    "$$\n",
    "\\beta^* = argmin_{\\beta}(||y-X^{t}\\beta||^{2}_{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biais vs Variance \n",
    "\n",
    "Nous souhaitons modéliser $Y$ en utilisant les variables explicatives $X$ et supposez qu'il existe une fonction $f$ qui représente la véritable relation entre $Y$ et $X$ de telle sorte que :\n",
    "\n",
    "$$\n",
    "Y=f(X)+\\epsilon\n",
    "$$\n",
    "\n",
    "| Voir | Classification vs Regression |\n",
    "|------|------------------------------|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__L'erreur quadratique moyenne__ peut être décomposée en termes de biais et de variance : \n",
    "\n",
    "$$\n",
    "E[(Y-\\widehat{f})^2]=bias^2+variance+\\sigma^2\n",
    "$$\n",
    "\n",
    "\n",
    "|               | Signification |\n",
    "|---------------|----------------------------|\n",
    "| $Y$       | variable cible (target) |\n",
    "| $\\hat{f}$ | fonction estimée de $f$ |\n",
    "| $biais$  | erreur carrée moyenne |\n",
    "| $variance$    | quantité de variation |\n",
    "| $\\sigma$ | $sigma^2 = \\epsilon$ (bruit) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle Ridge \n",
    "Le modèle Ridge est une version pénalisée du modèle linéaire multiple dont la fonction de coût est :\n",
    "\n",
    "$$\n",
    "||y-X^{t}\\beta||^{2}_{2}+\\lambda||\\beta||^{2}_{2},\\lambda\\in\\mathbb{R^{+*}}\n",
    "$$ \n",
    "\n",
    "* $\\lambda = 0$ : Variance au Max (Overfitting)\n",
    "* $\\lambda$ augmentent : Biais augmentent & Variance diminue\n",
    "* $\\lambda = \\infty$ : Biais au Max (Underfitting)\n",
    "\n",
    "_Ridge réduit les coefficients associés aux variables, mais il fonctionne de manière globale._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle Lasso \n",
    "Le modèle LASSO (Least Absolute Shrinkage and Selection Operator) dont la fonction de coût est :\n",
    "\n",
    "$$\n",
    "|||Y-X^{t}\\beta||^{2}_{2}+\\lambda|||\\beta||_{1}\n",
    "$$\n",
    "\n",
    "* $\\lambda$ augmentent : Biais augmentent & Variance diminue\n",
    "* $\\lambda$ diminue : Le nombre de coefficients non nuls augmentera (Overfitting)\n",
    "\n",
    "_Le modèle Lasso filtre les variables explicative les plus inutiles._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting and Overfitting\n",
    "\n",
    "* __Underfitting :__ est le fait qu'un modèle est trop simple pour renvoyer une bonne estimation de la variable cible.\n",
    "\n",
    "* __Overfitting :__ lorsqu'un modèle correspond trop bien aux données d'entraînement.\n",
    "\n",
    "![](https://datascience.foundation/img/pdf_images/underfitting_and_overfitting_in_machine_learning_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross validation\n",
    "\n",
    "Cette cross validation augmente notre nombre de ligne ($n$) en multipliant le nombre d'observation($k$).\n",
    "\n",
    "![overfitting](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search cross validation\n",
    "\n",
    "Cette cross validation combine plusieurs modèles de machine learning avec differents parametre.\n",
    "\n",
    "Grille de parametres :\n",
    "```\n",
    "grid = { \n",
    "    'n_estimators': [200,300,400,500],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy'],\n",
    "    'random_state' : [18]\n",
    "}\n",
    "```\n",
    "et teste toute les combinaisons (ici $4\\times2\\times5\\times2\\times1 = 80$ modèles testés)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régressions logistique (Classification)\n",
    "\n",
    "\n",
    "$$\n",
    "P(Y=1)=f(X)+\\epsilon\n",
    "$$\n",
    "\n",
    "|     | Signification |\n",
    "|-----|----------------------------|\n",
    "| $P$ | Probabilité que |\n",
    "| $Y$ | la variable Cible (target) |\n",
    "| $f$ | fonction |\n",
    "| $X$ | la matrice Explicative |\n",
    "| $\\epsilon$  | le bruit (Episilon) |\n",
    "\n",
    "## fonction logistique \n",
    "\n",
    "$$\n",
    "f(X) = \\mathbb{sigmoid}(X) = \\frac{1}{1+\\mathbb{exp}(-X)}\n",
    "$$\n",
    "\n",
    "dans lequel : \n",
    "\n",
    "$$\n",
    "X = \\beta_{0}+X_{1}\\beta_{1}+...+X_{p}\\beta_{p}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fonction Logistique (Sigmoid) :__\n",
    "\n",
    "![logistic_function](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/logistic_function.png)\n",
    "\n",
    "Y indique si notre resultat est vrai (Y = 1) ou faux (Y = 0).\n",
    "\n",
    "La courbe bleue représente la probabilité en fonction de la valeur de X.\n",
    "\n",
    "Le tout dans l'intervalle [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonction de coût \n",
    "\n",
    "En Supervised Learning, la machine cherche les paramètres de modèle qui minimisent la Fonction Coût.\n",
    "\n",
    "Dans le cas de la régression logstique la fonction de coût (Log Loss):\n",
    "\n",
    "$$\n",
    "Log Loss = \\sum_{i=0}^n -y_ilog(\\hat{y_i}) - (1-y_i)log(1-\\hat{y_i})\n",
    "$$\n",
    "\n",
    "|     | Signification |\n",
    "|-----|----------------------------|\n",
    "| $y_i$ | la variable Cible (target) |\n",
    "| $\\hat{y_i}$ | prédiction de la variable Cible |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de confusion \n",
    "\n",
    "|Confusion Matrix| Model Pediction : __Positive__ | Model Pediction : __Negative__ ||\n",
    "|---|---|---|---|\n",
    "|__Reality : Positive__ |True Positive (TP) |False Negative (FN)|$$ {Sensitivity : } \\\\ \\frac{TP}{TP+FN}$$|\n",
    "|__Reality : Negative__ |False Positive (FP)|True Negative (TN) |$$ {Specificity : } \\\\ \\frac{TP}{TN+FP}$$|\n",
    "||$$ {Precision : } \\\\ \\frac{TP}{TP+FP}$$|$$ {Neg. Predictive Value : } \\\\ \\frac{TN}{TN+FN}$$|$$ {Accuracy : } \\\\ \\frac{TP+TN}{TN+FP+FP+FN}$$|\n",
    "\n",
    "Accuracy : représente la proportion de predictions exacte produite par le modèle.\n",
    "\n",
    "True Positive Rate (TPR) : Sensitivity or Recall : Taux Positif réel\n",
    "\n",
    "False Positive Rate (FPR) : Fallout : Taux de Faux Positif : $\\frac{FP}{FP+TN}$\n",
    "\n",
    "Precision : $\\frac{TP}{TP+FP}$\n",
    "\n",
    "### Score F1 :\n",
    "Le Score F1 est égale à la moyenne harmonique entre le rappel et la précision pour une classe donnée de la variable cible.\n",
    "$$\n",
    "\\frac{Precision + Recall}{Precision \\times Recall}\n",
    "$$\n",
    "\n",
    "### Courbes ROC et AUC :\n",
    "\n",
    "__Courbe ROC__\n",
    " * Pour visualiser la performance d'un modèle de classification binaire dans son ensemble.\n",
    " * Cette courbe est obtenue en traçant le taux TPR en fonction du taux FPR. \n",
    " * Idéalement cette courbe vient épouser le coin en haut à gauche.\n",
    " * Si celle ci s'approche de la droite, la prediction n'a aucun interêt.\n",
    "\n",
    " ![](https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png)\n",
    "\n",
    "__AUC (Area Under the Curve) :__\n",
    "\n",
    " L'ASC est interprétée comme la probabilité que le modèle donne un score plus élevé à une observation positive sélectionnée au hasard qu'à une observation négative sélectionnée au hasard.\n",
    "\n",
    " Un ASC < 50% signifie que le modèle ne prévoit pas mieux que le hazard complet.\n",
    "\n",
    " ### GINI coefficient : \n",
    "\n",
    " L'ASC est également liée à l'indice GINI, qui décrit la dispersion statistique de la population et est largement utilisé en économie pour quantifier les inégalités.\n",
    "\n",
    "$$\n",
    "GINI = 2AUC -1\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forest\n",
    "\n",
    "## Arbre de decision (regression and classification) (CART)\n",
    "\n",
    "L’arbre de décision est un algorithme itératif qui, à chaque itération, va séparer les individus en k groupes (généralement k=2 et on parle d’arbre binaire) pour expliquer la variable cible.\n",
    "\n",
    "La première division (on parle aussi de split) est obtenue en choisissant la variable explicative qui permet la meilleure séparation des individus. Cette division donne des sous-populations correspondant au premier nœud de l'arbre.\n",
    "\n",
    "Le processus de split est ensuite répété plusieurs fois pour chaque sous-population (noeuds précédemment calculés) jusqu'à ce que le processus de séparation s’arrête."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Critère de division :__\n",
    " * Recevabilité : si les branches resultante ont des noeuds non-vides.\n",
    " * Hétérogénéité : on essaye de minimiser la sommes des heterogeneité.\n",
    " \n",
    "Hétérogénéité avec $Y$ quantitatif :\n",
    "\n",
    "$$\n",
    "H_k=\\frac{1}{Card(k)}\\cdot\\sum_{i\\in{k}}(y_i-\\underline{y_k})^2\n",
    "$$\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|----------------------------|\n",
    "| $H$ | hétérogénéité |\n",
    "| $_k$ | nœud |\n",
    "| $Card(k)$ | nombre d'éléments dans le nœud k |\n",
    "| $\\cdot$ | $\\times$ |\n",
    "| $\\sum_{i\\in{k}}$ | sommes de l'ensemble des observations du nœud  |\n",
    "| $y_i$ | la variable Cible (target) |\n",
    "| $\\underline{y_k}$ | moyenne des valeurs de $Y$ parmi les observations du nœud $k$ |\n",
    "\n",
    "Nous essayons de trouver la division qui minimise la somme de la variance de Y dans la branche droite et gauche. Plus la variance de Y dans chaque branche est faible, plus les observations dans les deux branches sont homogènes vis-à-vis de la variable cible, si l'on construit un modèle capable d'isoler des groupes d'observations qui ont des valeurs similaires de Y dans la même branche, alors il y a de fortes chances que ce soit un bon prédicteur !\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Hétérogénéité avec $Y$ qualitatif (ou GINI):\n",
    "\n",
    "$$\n",
    "H_k=\\sum_{i=1}^{m}p_{k}^{i}\\cdot(1-p_{k}^{i})\n",
    "$$\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|----------------------------|\n",
    "| $H$ | hétérogénéité |\n",
    "| $_k$ | nœud |\n",
    "| $m$ | nombre de modalités (catégories) |\n",
    "| $\\sum$ | somme |\n",
    "| $i=1$ | debut des iterations |\n",
    "| $p_{k}^{i}$ | proportion de la categorie $i$ de $Y$ dans le nœud $k$.|\n",
    "| $\\cdot$ | $\\times$ |\n",
    "\n",
    "Nous cherchons donc la division qui minimise la somme des hétérogénéités des nœuds enfants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Critère d'arrêt :__\n",
    " * homogène : lorsque toutes les observations dans le nœud ont la même valeur ou la même modalité de $Y$.\n",
    " * lorsqu'il n'y a plus de divisions recevables.\n",
    " * lorsq'il a atteint un cran d'arrêt prédéfini (```min_samples_split``` ou ```min_samples_leaf```).\n",
    " * lorsque nous atteignons une certaine profondeur (un certain nombre de subdivions répétées)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__lutter contre l' overfitting :__\n",
    "\n",
    "L'objectif est de trouver un arbre intermédiaire pour obtenir un compromis biais/variance en jouant avec les critères d'arrêt à l'aide de gridsearch :\n",
    "* ```min_samples_leaf```\n",
    "* ```min_samples_split```\n",
    "* ```max_depth```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "### __Bagging :__ contraction Bootstrap et Agrégation.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/512px-Ensemble_Bagging.svg.png)\n",
    "\n",
    "Étant donné un ensemble d'entraînement standard $D$ de taille $n$, le bagging génère $m$ nouveaux ensembles d'entrainement $D_i$, chacun de taille $n'$, par échantillonage uniforme. Il peut y avoir des doublons de variable observé d'un échantillon à l'autre. Appellé échantillon de Bootstrap. Puis, $m$ modèles sont entrainées à l'aide des $m$ échantillons de bootstrap. Pour finir, la prédiction du meta-modèle est obtenue en faisant la moyenne de la sortie (pour la régression) ou par vote de majorité (pour la classification) des $m$ modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Y quantitatif :__ le modèle agrégé est la moyenne des fonctions estimées par les modèles, la moyenne des valeurs de $Y$ pour une observation donnée.\n",
    "\n",
    "$$\n",
    "\\hat{f}_B(.)=\\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}_{z_b}(.)\n",
    "$$\n",
    "\n",
    "__Y qualitatif :__ le modèle agrégé est le vote majoritaire parmi les fonctions estimées par les modèles, la modalité de $Y$ les plus représentés parmi les réponses des différents modèles à une observation donnée.\n",
    "\n",
    "$$\n",
    "\\hat{f}_B(.)=arg\\max_{i}Card\\{b|\\hat{f}_B(.)=j\\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "__Mean decrease Accuracy :__ Cette méthode consiste à échanger aléatoirement les valeurs d'une variable explicative. La différence entre l'erreur de validation avant et après le changement est ensuite mesurée. Plus l'erreur de validation pré-commutation est élevée, plus la variable en question est considérée comme importante pour la prédiction de la variable cible.\n",
    "\n",
    "__Mean decrease Gini :__ Cette méthode permet d'évaluer l'importance d'une variable au niveau d'un nœud : elle mesure la diminution de la fonction d'hétérogénéité si nous réutilisons la variable explicative utilisée pour le nœud par celle que nous voulons évaluer. L'importance générale de la variable est alors une somme des diminutions de l'hétérogénéité mesurées et pondérées par le nombre d'observations à chaque nœud.\n",
    "\n",
    "__Controle du bais et de la variance :__ Les arbres de décision cessent naturellement de diviser le jeu de données lorsque toutes les branches contiennent des sous- ensembles de données dans lesquels la variable cible est homogène (la même valeur pour toutes les observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "Les SVM sont des modèles adaptés aux problèmes de classification. Ils cherchent à construire la limite la plus éloignée possible entre les observations appartenant aux différentes classes.\n",
    "\n",
    "__Linearl kernel :__\n",
    "\n",
    "L'idée sous-jacente est de séparer les points appartenant à différentes classes en trouvant un hyperplan. \n",
    "\n",
    "les observations sont représentées sous la forme d'un nuage de points dans un $d$ espace dimensionnel où $d$ est le nombre de variables explicatives que nous utilisons pour la modélisation.\n",
    "\n",
    "Nous calculons la distance entre les vecteurs de support et la ligne de séparation, cette distance est appelée la marge. Le but de Support Vector Machines est de maximiser la marge, comme le montre l'exemple ci-dessous :\n",
    "\n",
    "![](https://www.researchgate.net/profile/Seyyid-Ahmed-Medjahed/publication/282464007/figure/fig1/AS:296526237716481@1447708772146/The-optimal-Hyperplane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear SVM (Kernel-based estimators)\n",
    "\n",
    "__Polynomial kernel :__\n",
    " \n",
    "nous pouvons produire une nouvelle dimension grâce à un grain polynomial $k$, qui est la fonction du noyau polynomial. Cette nouvelle dimension n'est pas une combinaison linéaire des variables explicatives et augmente donc la dimension de l'espace d'observation de deux à trois.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Alexander-Smola/publication/229025155/figure/fig4/AS:300828989640727@1448734628993/Example-of-an-SV-classifier-found-using-a-radial-basis-function-kernel-k-x-x-exp.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Noyau radial :__\n",
    "\n",
    "Dans le cas d'un groupe entouré d'un second, ce problème peut être transformé en un problème linéairement séparable dans un espace plus vaste. Si nous ajoutons une troisième dimension $z = x^2 + y^2$ qui représente la distance entre un point et l'origine du système de coordonnées orthonormal\n",
    "\n",
    "![](https://i.stack.imgur.com/qyU9m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__lutter contre l' overfitting :__\n",
    "\n",
    "Pour les modèles SVM, comme pour tout modèle avec une variance naturellement élevée et un biais faible, il est essentiel de trouver un bon compromis de variance de biais afin d'éviter un ajustement excessif :\n",
    "* C penalty : \n",
    "    * Pour de grandes valeurs de C, l'optimisation choisira un hyperplan à plus petite marge si cet hyperplan fait un meilleur travail pour classer correctement tous les points d'entraînement. \n",
    "    * Pour de petite valeur de C amènera l'optimiseur à rechercher un hyperplan séparant une plus grande marge, même si cet hyperplan classe à tort plus de points.\n",
    "* Gamma parameter : \n",
    "    * Le paramètre gamma est une mesure de la portée de l'influence de chaque observation lors de la construction de la frontière de décision, des valeurs élevées de gamma se traduisent par des zones d'influence très petites, lorsque le gamma est faible (proche de zéro), la zone d'influence de chaque observation est très grande.\n",
    "    * Gamma : 0 = Overfitting  variance élevé.\n",
    "    * Gamma : $\\infty$ =  Underfitting biais élevé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting - ADABOOST\n",
    "\n",
    "Le boosting permet d'améliorer les performances d'un modèle simple d'apprentissage automatique supervisé en répétant l'étape d'apprentissage plusieurs fois de suite tout en tenant compte des erreurs de prédiction, puis en combinant les différents modèles. Cette méthode utilise la notion d'agrégation.\n",
    "\n",
    "La méthode AdaBoost construit  des modèles successivement amélioré en modifiant les poids attribués aux observations. Elle augmente l'importance de celles qui causent des erreurs et minimise l'importance de celles qui sont correctement classées par le modèle. Au final, les modèles sont agrégés, et le poids de chaque modèle est retenue en fonction de leur erreur d'apprentissage.\n",
    "\n",
    "__le modèle final obtenu à partir de l'algorithme AdaBoost :__\n",
    "\n",
    "\n",
    "$$H(X)=sign(\\sum_{t=1}^{T}\\alpha_t{h_t}(X))=sign(F(x))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditions de performance \n",
    "\n",
    "* Il doit avoir été formé sur un nombre suffisant d'exemples\n",
    "* Il doit être capable de décrire avec précision les observations de la base de formation.\n",
    "* Il doit être simple\n",
    "\n",
    "__Deuxième critère de performance__\n",
    "* communément appelé condition d'apprentissage faible, correspond à l'idée que chaque modèle de classification binaire qui constitue le modèle final a une erreur d'entraînement qui est plus petite que le pur hasard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting AdaBoost \n",
    "\n",
    "Une fois ces deux conditions vérifiées, l'algorithme AdaBoost réduit l'erreur d'apprentissage à un niveau minimum en relativement peu d'itérations.\n",
    "\n",
    "Cependant, comme c'est souvent le cas en statistique, une erreur minimale d'entraînement signifie souvent un surajustement\n",
    "\n",
    "__Marges__ \n",
    "\n",
    "Il existe un cadre théorique dans lequel l'algorithme AdaBoost fournit un modèle où l'erreur sur l'échantillon de validation atteint un niveau optimal, et où le modèle produit s'améliore au fur et à mesure des itérations.\n",
    "\n",
    "La marge réagit comme un système de vote qui fait davantage confiance à une décision majoritaire.\n",
    "\n",
    "La marge est une valeur qui fluctue dans la plage [-1.1] qui est définie comme suit:\n",
    "\n",
    "$$\n",
    "Y_{i}F(x)=\\sum_{t:Y_i=h_t(X_i)}\\alpha_t\\;-\\sum_{t:Y_i\\neq{h_t}(X_i)}\\alpha_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting (XGBoost)\n",
    "\n",
    "XGBoost (ou contraction de eXtreme Gradient Boosting) est un modèle de machine Learning basé sur l'apprentissage d'ensemble séquentiel et les arbres de décision. Comme son nom l'indique, il utilise le boosting de gradient.\n",
    "\n",
    "Considérons un classifieur faible initial $f0$. Après l'avoir optimisé, la méthode de boosting cherche à construire un nouveau classifieur faible $f1$ à partir de $f0$ en introduction un terme de résidu $h$ :\n",
    "$$\n",
    "f_1(x) = f_0(x) + h(x)\n",
    "$$\n",
    "\n",
    "De sorte que $f1$ soit plus performant que $f0$. On répétant l'opération un certain nombre de fois, disons $p$, on construit un classifieur final $F$ complexe qui est une combinaison linéaire des $f_i$, où chacun des \n",
    "$f_i$ est associé à un poids $\\alpha i$ :\n",
    "\n",
    "$$\n",
    "F(x) = \\sum^n_{i=1} \\alpha _i f_i(x)\n",
    "$$\n",
    "\n",
    "Le Gradient Boosting est une technique particulièrement puissante dans le cas où la fonction de perte est différentiable.\n",
    "\n",
    "L'apprentissage d'un XGBoost est plus long que les autres modèles de ML mais est considéré comme le meilleur de sa catégorie pour les données structurées/tabulaires de petite à moyenne taille.\n",
    "\n",
    "[Info](https://blent.ai/xgboost-tout-comprendre/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "## Voting / Averaging\n",
    "\n",
    "Nous parlons de \"Voting classifier\" lorsque la variable cible est qualitative.\n",
    "nous formerons plusieurs modèles différents (par exemple, un Bayes naïf, un SVM et une forêt aléatoire). Chacun des modèles donnera une certaine prédiction $\\hat{y}_1$, $\\hat{y}_2$, $\\hat{y}_3$ et un certain score  $L1$, $L2$, $L3$. Le classificateur de vote est un modèle agrégé dont les prédictions sont calculées par un vote pondéré des différents modèles.\n",
    "\n",
    "Qui peut être écrit :\n",
    "\n",
    "$$\n",
    "\\hat{y} = {argmax}_{m \\epsilon M} ( S_1 \\times (\\hat{y}_1 = m) +  S_2 \\times (\\hat{y}_2 = m) +  S_3 \\times (\\hat{y}_3 = m) )\n",
    "$$\n",
    "\n",
    "|             | Signification |\n",
    "|-------------|----------------------------|\n",
    "| $\\hat{y}$ | prédiction  |\n",
    "| $m$ | modalité (catégories) |\n",
    "| $M$ | Ensemble des catégories |\n",
    "| $S$ | Score |\n",
    "\n",
    "Chacun de ces modèles donne une prédiction pour $y$ : $\\hat{y}_1$, $\\hat{y}_2$, $\\hat{y}_3$ et un certain score : $S1$, $S2$, $S3$. La prédiction finale sera la moyenne pondérée en fonction du score des prédictions des modèles utilisés.\n",
    "\n",
    "Qui peut être écrit :\n",
    "\n",
    "$$\n",
    "\\hat{y}=\\frac{1}{S_1+S_2+S_3}\\times(S_1\\times\\hat{y}_1+S_2\\times\\hat{y}_2+S_3)\n",
    "$$\n",
    "\n",
    "|             | Signification |\n",
    "|-------------|----------------------------|\n",
    "| $\\hat{y}$ | prédiction  |\n",
    "| $S$ | Score |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Le stacking (ou dit parfois blending) est un procédé qui consiste à appliquer un algorithme de machine learning à des classifieur générés par un autre algorithme de machine learning.\n",
    "\n",
    "D’une certaine façon, il s’agit de prédire quels sont les meilleurs classifieurs et de les pondérer. Cette démarche a l’avantage de pouvoir agréger des modèles très différents et d’améliorer sensiblement la qualité de la prédiction finale.\n",
    "\n",
    "Pour pouvoir évaluer les différents modèles à stacker, on utilise des poids constants permettant de pondérer les différents modèles. C’est du stacking linéaire standard.\n",
    "\n",
    "La principale limitation est qu’on a perdu la dépendance des modèles à leur data et plus particulièrement leur spécialisation. On va donc utiliser des poids variables qui dépendent des données ; soit des méta-features. Le stacking est cette fois-ci dépendant du poids des features.\n",
    "\n",
    "[info](https://blog.octo.com/les-methodes-ensemblistes-pour-algorithmes-de-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothèses et terminologie\n",
    "\n",
    "__Apprentissage supervisé :__ Machine learning avec une cible (target)\n",
    "\n",
    "__Classification :__ prédiction de valeur qualitative \n",
    "\n",
    "__Accuracy :__ le nombre de prédictions précises divisées par le nombre d'observations. (0-1 Loss)\n",
    "\n",
    "__R carré :__ pour les modèles de régression, la mesure d'évaluation la plus couramment utilisée est R-carré qui est égal à : \n",
    "$Rsquares = 1 - \\frac{SSR}{SST}$ Où $SSR$ est la somme des résidus carrés. \n",
    "\n",
    "__Biais :__ ${Biais} = E[\\hat\\beta]-\\beta$\n",
    "\n",
    " * Où $E[\\hat\\beta]$ est la valeur attendue de $\\hat\\beta$ et $\\beta$ est la valeur réelle du paramètre considéré.\n",
    "\n",
    "__Variance :__ La variance est la variance statistique de l'estimateur qui peut être écrite : \n",
    " * $Variance = E[(E[\\hat{\\beta}]-\\hat{\\beta})^2]$\n",
    "\n",
    "__Target function :__ $f(x)=y$.\n",
    "\n",
    "__Model :__ Dans l'apprentissage automatique, le modèle est une fonction, souvent notée $h$ (modèle ML == l'hypothèse) est une fonction qui, nous l'espérons, ressemble à la fonction cible inconnue $f$.\n",
    "\n",
    "__stratify :__ Lors de la repartition en divers set train et test, les proportions de valeurs sont respecté.\n",
    "\n",
    "## Holdout Validation \n",
    " * Divisez aléatoirement l'ensemble de données en un ensemble de trains et un ensemble d'essais.\n",
    " * Choisir un modèle qui pourraient être un bon prédicteur pour la variable cible. Une boucle externe peut être mise en place pour optimiser les hyperparemètres du modèle\n",
    " * estimer les performances générales du modèle entraîné en utilisant les données de l'ensemble de tests.\n",
    "* nous avons une estimation de la performance de notre modèle sur des données invisibles.\n",
    "\n",
    "_La méthode de maintien a un défaut principal, elle modifie la distribution des données en raison d'un échantillonnage sans remplacement, qui peut être partiellement corrigé par stratification (Biais pessimiste)._\n",
    "\n",
    "## Repeated Hold out method\n",
    "\n",
    "__Motivation__\n",
    "\n",
    "Toute erreur de modèle peut être décomposée en trois valeurs différentes : biais $b^2$, variance $\\sigma^2$, et le bruit $\\epsilon^2$\n",
    "$ {Error} = b^2 + \\sigma^2 + \\epsilon^2 $\n",
    "\n",
    "Le bruit ne peut pas être prédit parce qu'il est complètement distribué au hasard.\n",
    "\n",
    "Un faible biais caractérise un modèle précis. La variance mesure la sensibilité des prédictions du modèle aux petits changements dans l'entrée, elle mesure en fait la variance des prédictions. Une faible variance caractérise un modèle précis.\n",
    "\n",
    "![bias_variance](https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Machine%20Learning%20Supervis%C3%A9/model_evaluation/bias_variance.PNG)\n",
    "\n",
    "N'oubliez pas que le biais et la variance d'un modèle ont des comportements de variation opposés, si vous souhaitez réduire la variance, vous augmenterez automatiquement le biais de votre modèle et vice versa.\n",
    "\n",
    "le fractionnement des données entre un ensemble de trains et un ensemble d'essais, qui est un sous-échantillonnage sans remplacement, modifie la distribution des données et que ces modifications sont inversement proportionnelles à la taille de l'échantillon original.\n",
    "\n",
    "![subsample](https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Machine%20Learning%20Supervis%C3%A9/model_evaluation/subsampling_effect.PNG)\n",
    "\n",
    "__Repeated Hold out__\n",
    "\n",
    "Pour faire face à cette difficulté, nous pouvons nous appuyer sur une méthode appelée hold-in répétée, qui consiste simplement à répéter la méthode hold-in-in un certain nombre de fois afin d'évaluer la performance générale d'un modèle\n",
    "\n",
    "## Booststrap method\n",
    "\n",
    "Le principal problème avec la précision de la resubstitution booststrap est le biais extrêmement optimiste qu'elle génère\n",
    "\n",
    "## Cross-Validation and Hyperparameter Optimization\n",
    "\n",
    "### k-fold cross-validation for model evaluation\n",
    "\n",
    "La méthode de validation croisée du pli k consiste à diviser le jeu de données en k parties. Chaque partie jouera également le rôle de l'ensemble de tests tandis que les autres parties seront réunies pour former l'ensemble d'entraînement. Le modèle est ensuite formé pour chacune de ces k configurations et la performance est mesurée sur les k ensembles de tests correspondants.\n",
    "\n",
    "### Model Selection via k-fold cross validation\n",
    "\n",
    "Semblable à la méthode de résistance, nous divisons l'ensemble de données en deux parties. Pour chaque configuration d'hyperparamètre, nous appliquons la méthode de validation croisée k-fold sur le set de formation. En prenant les paramètres d'hyperparamètre qui ont produit les meilleurs résultats dans la procédure de validation croisée k-fold. nous utilisons le jeu de tests indépendant de tests pour évaluer le modèle que nous avons obtenu. Enfin, nous pouvons éventuellement adapter un modèle à toutes les données ce qui pourrait être le modèle pour le déploiement.\n",
    "\n",
    "### 3-Way hold out method for Large Datasets\n",
    "\n",
    "Nous commençons par diviser notre ensemble de données en trois parties, un ensemble de formation pour le montage du modèle, un ensemble de validation pour la sélection du modèle et un ensemble de tests pour l'évaluation finale du modèle sélectionné. Nous utilisons l'algorithme d'apprentissage avec différents paramètres d'hyperparamètre pour adapter les modèles aux données d'entraînement. Nous évaluons la performance de nos modèles sur le jeu de validation. nous pouvons fusionner le jeu de formation et de validation après la sélection du modèle et utiliser les meilleurs paramètres d'hyperparamètre de l'étape précédente pour adapter un modèle à cet ensemble de données plus vaste. Nous pouvons fusionner le jeu de formation et de validation après la sélection du modèle et utiliser les meilleurs paramètres d'hyperparamètre de l'étape précédente pour adapter un modèle à cet ensemble de données plus vaste. Nous pouvons utiliser toutes nos données - fusionnant l'entraînement et l'ensemble de tests - et adapter un modèle à tous les points de données pour une utilisation dans le monde réel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of data and cross-validation\n",
    "\n",
    "Au cours de la validation croisée, plusieurs fractionnements de test de train se produisent, par conséquent une normalisation spécifique doit être appliquée dans chaque cas afin d'éviter une fuite des informations contenues dans le jeu de test spécifique vers le train. Cela pourrait conduire à des problèmes de surajustement et biaiser l'évaluation de nos modèles. Une solution consiste à regrouper tout votre prétraitement et votre modèle dans un pipeline avant d'appliquer GridSearchCV pour régler vos hyperparamètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Times series\n",
    "\n",
    " L'analyse des séries chronologiques tient compte du fait que les points de données pris au fil du temps peuvent avoir une structure interne (telle que l'autocorrélation, la tendance ou la variation saisonnière) qui devrait être prise en compte.\n",
    "\n",
    " ## Décomposition\n",
    "\n",
    " ![](https://machinelearningmastery.com/wp-content/uploads/2017/01/Additive-Model-Decomposition-Plot.png)\n",
    "\n",
    " __Level :__ moyenne (average) de la série temporel (TS)\n",
    "\n",
    " __Trend :__ la pente de la série.\n",
    "\n",
    " __Seasonality :__ La répétition cyclique dans la série. \n",
    "\n",
    " __Noise :__ la variation aléatoire. \n",
    "\n",
    " ## Smooting techniques\n",
    "\n",
    " Il existe des méthodes pour réduire ou annuler l'effet dû à une variation aléatoire.\n",
    "\n",
    " ### Averaging(Moyenne)\n",
    "\n",
    " des techniques de calcul qui consistent à prédire les valeurs futures ou à estimer la tendance générale de la série en utilisant une sorte de moyenne des valeurs mesurées.\n",
    "\n",
    "### Single moving average\n",
    "\n",
    " Une autre façon de résumer les données passées est de calculer la\n",
    "moyenne des petits ensembles successifs de plus petits nombres de\n",
    "Xt+Xt−1+... +Xt−T+1 données passées comme suit : \n",
    "\n",
    "$$M_t = \\frac{X_t+X_{t-1}+ ... +X_{t-T+1}}{T}$$\n",
    "\n",
    "|             | Signification |\n",
    "|-------------|----------------------------|\n",
    "| $t$ | la date observé  |\n",
    "| $X_t$ | la valeur de la série chronologique observée à la date $t$ |\n",
    "| $T$ | le nombre de dates observées par la moyenne mobile |\n",
    "\n",
    "les Single moving average sont bien meilleures pour suivre la tendance générale des données en fonction de l'ordre sélectionné.\n",
    "\n",
    "### Exponential Smoothing\n",
    "\n",
    "le lissage exponentiel attribue des pondérations décroissantes exponentiellement à mesure que l'observation vieillit.\n",
    "\n",
    "### Double exponential smoothing\n",
    "\n",
    "le lissage unique n'excelle pas dans le suivi des données lorsqu'il y a une tendance. Cette situation peut être améliorée par l'introduction d'une seconde équation avec une seconde constante,$\\gamma$, qui doit être choisi en conjonction avec $\\alpha$.\n",
    "\n",
    "\n",
    "$$S_t = \\alpha X_t + (1-\\alpha)(S_{t-1} + b_{t-1}) \\; \\alpha \\in [0,1]$$\n",
    "$$b_t = \\gamma(S_t - S_{t-1}) + (1-\\gamma)b_{t-1} \\; \\gamma \\in [0,1$$\n",
    "\n",
    "### Triple exponential smoothing \n",
    "\n",
    "Si les données montrent une tendance et une saisonnalité, le double lissage ne fonctionnera pas. Nous introduisons maintenant une troisième équation pour prendre soin de la saisonnalité (parfois appelée périodicité). L'ensemble d'équations résultant est appelé la méthode \"Holt-Winters\" (HW) d'après les noms des inventeurs. Les équations de base de leur méthode sont données par:\n",
    "\n",
    "$$S_t = \\alpha \\frac{X_t}{I_{t-T} + (1-\\alpha)(S_{t-1} + b_{t-1})} \\;\\; OVERALL \\;\\; SMOOTHING$$\n",
    "$$b_t = \\gamma(S_t - S_{t-1}) + (1-\\gamma)b_{t-1} \\;\\; TREND \\;\\; SMOOTHING $$\n",
    "$$I_t = \\beta \\frac{X_t}{S_t} + (1-\\beta)I_{t-T} \\;\\; SEASONAL \\;\\; SMOOTHING $$\n",
    "$$F_{t+m} = (S_t + m b_t)I_{t-T+m} \\;\\; FORECAST$$\n",
    "\n",
    "|             | Signification |\n",
    "|-------------|----------------------------|\n",
    "| $X$ | l'observation   |\n",
    "| $S$ | l'observation lissée |\n",
    "| $b$ | le facteur de tendance (trend) |\n",
    "| $I$ | la saisonnalité |\n",
    "| $F$ | la prévision à $m$ dates à venir |\n",
    "| $t$ | les données |\n",
    "| $T$ | la période de saisonnalité potentielle |\n",
    "\n",
    "## Modélisation de séries chronologiques univariées\n",
    "\n",
    "### Stationarity\n",
    "Un processus stationnaire a la propriété que la structure moyenne, variance et autocorrélation ne changent pas avec le temps.\n",
    "\n",
    "### Bruit blanc\n",
    "Un processus stochastique en temps discret $A_t$ un processus stochastique à temps discret qui signifie qu'il ne dépend pas du tempstet est égal à zéro\n",
    "\n",
    "### Modèles autorégressifs (AR)\n",
    "Un modèle autorégressif est simplement une régression linéaire de la valeur actuelle de la série par rapport à une ou plusieurs valeurs antérieures de la série.\n",
    "\n",
    "### Modèles de moyenne mobile\n",
    "un modèle de moyenne mobile est conceptuellement une régression linéaire de la valeur actuelle de la série par rapport au bruit blanc.\n",
    "\n",
    "### Box-Jenkins ARIMA\n",
    "Le modèle Box-Jenkins ARIMA est une combinaison des modèles AR et MA :\n",
    "$$\n",
    "D_t = \\delta + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + ... + \\phi_{p}X_{t-p} + A_t - \\theta_1 A_{t-1} -\\theta_2 A_{t-2} - ... - \\theta_q A_{t-q}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

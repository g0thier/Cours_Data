{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning \n",
    "\n",
    "__Suppervised learning__\n",
    " * Classification\n",
    " * Regression\n",
    "\n",
    "__Reinforcement learning__ \n",
    " * Exemple : AI de jeu-video, Navigation Tesla, Decisions temps-réel.\n",
    "\n",
    "__Unsupervised learning__ \n",
    " * Reduction (Dimensionally reduction)\n",
    " * Regroupement (Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification vs Regression \n",
    "\n",
    "__Classification__\n",
    " * Prédire une catégorie. \n",
    "\n",
    "__Regression__\n",
    " * Prédire une valeur numérique (Quantitative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces problèmes peuvent etre représenter de la manière suivante :\n",
    "\n",
    "$$Y = f(X) + \\epsilon$$\n",
    "\n",
    "| | Signification |\n",
    "|-----|----------------------------|\n",
    "| $Y$ | la variable Cible (target) |\n",
    "| $X$ | les valeurs Explicatives : |\n",
    "|  | $n$ : lignes|\n",
    "|  | $p$ : colonnes |\n",
    "| $f$ | fonction |\n",
    "| $\\epsilon$ | le bruit (Episilon) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valeurs catégorique et numérique \n",
    "\n",
    "| Valeurs |  | Signification |\n",
    "|--------------------------|-----------|--------------|\n",
    "| Quantitative (Numérique) | Continue | Float |\n",
    "|                          | Discrète | Entier ou notion de pas |\n",
    "| Qualitative (Catégoriel) | Ordinales | Note A, B, C |\n",
    "|                          | Nominales | Sexe F ou M |\n",
    "\n",
    "\n",
    "$X$ est une valeur explicative.\n",
    "\n",
    "$Y$ est une valeur cible (target). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préprocesing : Netoyage \n",
    "__Supression de colonnes__\n",
    " * Valeurs uniques : ID, Nom, Siret...\n",
    " * Trop tronquées : Plus de 60 % de valeur manquante. \n",
    " * Valeurs Nominales excessives : 36 categories.\n",
    " * Une des deux valeurs colineaires : Male 0 . 1 & Femal 1 . 0.\n",
    "\n",
    " __Supression de lignes__\n",
    "  * Valeurs cible manquante (Y).\n",
    "  * Les valeur aberrante (Outlier).\n",
    "\n",
    "### Complétions des valeurs manquantes \n",
    "__Valeurs Quantitative__\n",
    " * Moyenne(mean) : parfois sur les valeurs continues.\n",
    " * Mediane(median) : à utiliser en priorité.  \n",
    "\n",
    "__Valeurs Qualitatives__\n",
    " * valeur la plus fréquente (mode).\n",
    " * remplacer par un mot clef comme Missing Value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation \n",
    "__Normalisation :__\n",
    "- Notre valeur $X$ est remplacé par la valeur de cette formule : \n",
    "\n",
    "$$\\frac{X - \\bar{X}}{\\sqrt(Var(X))}$$\n",
    "\n",
    "|                 | Signification |\n",
    "|-----------------|-----------------------------------------|\n",
    "| $X - \\bar{X}$   | Supression de la moyenne de la variable |\n",
    "| ––––––––––––––– | divisé par |\n",
    "| $\\sqrt(Var(X))$ | L'écart type de la variable |\n",
    "\n",
    "Ce qui entraîne une variable de moyenne 0 et un écart type (standard deviation) de 1.\n",
    "\n",
    "\n",
    "_A noter que la normalisation se fait sur les valeurs explicatives ($X$) mais jamais sur la cible ($Y$)_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage \n",
    "__One Hot Encoding (or Dummy Encoding)__\n",
    "\n",
    "Les valeurs qualitatives (catégoriel) sont ecrasé en colonnes binarisé. \n",
    "\n",
    "| Pays    |\n",
    "|---------|\n",
    "| France  |\n",
    "| Espagne |\n",
    "| Italie  |\n",
    "\n",
    "Devient : \n",
    "\n",
    "| France | Espagne | Italie |\n",
    "|--------|---------|--------|\n",
    "|1|0|0|\n",
    "|0|1|0|\n",
    "|0|0|1|\n",
    "\n",
    "Puis décolinéarisé :\n",
    "\n",
    "| Espagne | Italie |\n",
    "|---------|--------|\n",
    "|0|0|\n",
    "|1|0|\n",
    "|0|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régressions linéaires (Régression)\n",
    "$$\n",
    "Y = \\beta_{0} + \\beta_{1}X_1 + \\epsilon\n",
    "$$\n",
    "\n",
    "|             | Signification |\n",
    "|-------------|----------------------------|\n",
    "| $Y$         | la variable Cible (target) |\n",
    "| $X_1$       | la variable Explicatives |\n",
    "| $\\beta_{0}$ | Constante, là ou commence la droite à $X_1 = 0$ |\n",
    "| $\\beta_{1}$ | Coefficient directeur (pente / slope) |\n",
    "| $\\epsilon$  | le bruit (Episilon) |\n",
    "\n",
    "$Y =  ax + b + \\epsilon$ (erreur)\n",
    "\n",
    "Pour que la regression lineaire soit vrai :\n",
    "\n",
    "* __Linearity :__ les points doivent suivre une droite.\n",
    "* __Homoscedasticity :__ l'erreur $\\epsilon$ doit etre constante et ne pas suivre l'echelle de $X_1$.\n",
    "* __Independence of residuals :__ les erreurs ne doivent pas etre corrélés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation : Méthode des moindres carrés ordinaire\n",
    "_ordinary least square method est une fonction de perte_\n",
    "\n",
    "![](https://user.oc-static.com/upload/2018/11/14/15421935105892_P2C2_Graphique1.jpg)\n",
    "\n",
    "$$\n",
    "Min (\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2)\n",
    "$$\n",
    "\n",
    "En décomposant $\\hat{y}$, on obtient :\n",
    "\n",
    "$$\n",
    "Min (\\sum_{i=1}^{n}(y_{i}-\\beta_{0}-\\beta_{1}x_{i})^2)\n",
    "$$\n",
    "\n",
    "|               | Signification |\n",
    "|---------------|----------------------------|\n",
    "| $y_{i}$       | position reel du point |\n",
    "| $\\hat{y}_{i}$ | estimation de point de la regression lineaire |\n",
    "|               | $ax_{i} + b$ |\n",
    "| $\\sum$    | somme |\n",
    "| $n$ | ensemble n |\n",
    "| $i$ | debut des iterations |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régressions linéaires multiple \n",
    "\n",
    "Une première étape est de __normaliser__ les resultats pour leur donner une valeur moyenne de 0 et un écart type de 1, ainsi les valeurs explicatives sont comparable. \n",
    "\n",
    "Cette formule correspond à la précedente pour de multiple valeurs explicatives :\n",
    "\n",
    "$$\n",
    "Y_{i} = \\beta_{0}+X_{i,1}\\beta_{1}+...+X_{i,p}\\beta_{p}+\\epsilon_{i}\\forall i \\in \\left [1,n \\right ]\n",
    "$$\n",
    "\n",
    "Qui peut etre résumé sous forme matriciel : \n",
    "\n",
    "$$\n",
    "Y=X \\times \\beta+\\epsilon\n",
    "$$\n",
    "\n",
    "|               | Signification |\n",
    "|---------------|----------------------------|\n",
    "| $Y$ | Vecteur de dimention $(n, 1)$ |\n",
    "| $X$ | Matrice de dimention $(n, p + 1)$ |\n",
    "| $\\beta$ | Vecteur de dimention $(p + 1, 1)$ |\n",
    "| $\\epsilon$ | Vecteur de dimention $(n, 1)$ |\n",
    "\n",
    "\n",
    "_1 : Il faut impérativement éviter les variables explicatives colinéaire_\n",
    "\n",
    "_2 : Il faut supprimer les valeurs aberante pour ne pas biaiser l'erreur ($\\epsilon$)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation de modèles\n",
    "\n",
    "### Analyse de la variance (ANOVA)\n",
    "\n",
    "![](https://i.stack.imgur.com/FOzPq.png)\n",
    "\n",
    "__SST : Sum of Square Total :__ \n",
    "- Indicateur de dispersion des valeurs de $Y$ (variable cible ou target) \n",
    "\n",
    "$$\n",
    "SST = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^2\n",
    "$$\n",
    "\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|----------------------------|\n",
    "| $y_{i}$ | une des variable cible |\n",
    "| $\\bar{y}$ | moyenne des variable cible |\n",
    "| $\\sum$ | somme |\n",
    "| $n$ | ensemble n |\n",
    "| $i=1$ | debut des iterations |\n",
    "\n",
    "_C'est la différence au carré entre la variable dépendante observée et sa moyenne._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__SSR : Sum of Squares due to Regression :__ \n",
    "- une mesure qui décrit dans quelle mesure notre prediction correspond aux données.\n",
    "\n",
    "$$\n",
    "SSR =\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2\n",
    "$$\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|----------------------------|\n",
    "| $y_{i}$ | une des variable cible |\n",
    "| $\\hat{y}_{i}$ | estimation de point de la regression lineaire |\n",
    "| $\\sum$ | somme |\n",
    "| $n$ | ensemble n |\n",
    "| $i=1$ | debut des iterations |\n",
    "\n",
    "\n",
    "_C'est la somme des différences au carré entre la valeur prédite et la moyenne de la variable dépendante._\n",
    "\n",
    "_Entre autre obtenir le plus petit SSR possible revient à appliquer la méthode des moindres carrés._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__SSE : Sum of Squares Error :__\n",
    "- une mesure qui représente la quantité de dispersion de la variable cible.\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=1}^{n}(\\hat{y}_{i}-\\bar{y})^2\n",
    "$$\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|----------------------------|\n",
    "| $\\hat{y}_{i}$ | une des variable cible predicte |\n",
    "| $\\bar{y}$ | moyenne des variable cible dépendante |\n",
    "| $\\sum$ | somme |\n",
    "| $n$ | ensemble n |\n",
    "| $i=1$ | debut des iterations |\n",
    "\n",
    "_C'est la somme des différences moyennes au carré entre les estimations du modèle pour chaque observation et la moyenne de la variable cible pour la population d'intérêt._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Resumé__\n",
    "* SST est la quantité totale d'informations.\n",
    "* SSE est l'information expliquée par le modèle.\n",
    "* SSR est l'information qui reste à expliquer, ou l'erreur commise.\n",
    "\n",
    "$$SST = SSR + SSE$$\n",
    "\n",
    "ou \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n}(y_{i}-\\bar{y})^2 = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2 + \\sum_{i=1}^{n}(\\hat{y}_{i}-\\bar{y})^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__R2 : R-carré :__\n",
    "- une statistique qui quantifie le pouvoir explicatif du modèle par rapport à la variable cible.\n",
    "si le modèle permet d'expliquer fidèlement la variable cible, alorsSSRsera plus proche de 0 etR2sera plus proche de 1.\n",
    "\n",
    "\n",
    "$$\n",
    "R^2 = 1-\\frac{SSR}{SST}\n",
    "$$\n",
    "\n",
    "ou\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{SSE}{SST}\n",
    "$$\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|------------------------------|\n",
    "| $SSE$ | Sum of Squares Error |\n",
    "| $SSR$ | Sum of Squares due to Regression |\n",
    "| $SST$ | Sum of Square Total |\n",
    "\n",
    "\n",
    "_C'est la somme des différences moyennes au carré entre les estimations du modèle pour chaque observation et la moyenne de la variable cible pour la population d'intérêt._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression\n",
    "\n",
    "* $n$ le nombre de d'observation/échantillons.\n",
    "* $p$ le nombre de variables explicatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonction de coût \n",
    "\n",
    "En Supervised Learning, la machine cherche les paramètres de modèle qui minimisent la Fonction Coût.\n",
    "\n",
    "La fonction de coût pénalise la distance d’un point où la prédiction est mauvaise à la frontière entre deux classes.\n",
    "\n",
    "La fonction de coût (cost function):\n",
    "\n",
    "$$\n",
    "||y-X^{t}\\beta||^{2}_{2}\n",
    "$$\n",
    "\n",
    "L'erreur quadratique moyenne (Mean Square Error): \n",
    "\n",
    "$$\n",
    "\\beta^* = argmin_{\\beta}(||y-X^{t}\\beta||^{2}_{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biais vs Variance \n",
    "\n",
    "Nous souhaitons modéliser $Y$ en utilisant les variables explicatives $X$ et supposez qu'il existe une fonction $f$ qui représente la véritable relation entre $Y$ et $X$ de telle sorte que :\n",
    "\n",
    "$$\n",
    "Y=f(X)+\\epsilon\n",
    "$$\n",
    "\n",
    "| Voir | Classification vs Regression |\n",
    "|------|------------------------------|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__L'erreur quadratique moyenne__ peut être décomposée en termes de biais et de variance : \n",
    "\n",
    "$$\n",
    "E[(Y-\\widehat{f})^2]=bias^2+variance+\\sigma^2\n",
    "$$\n",
    "\n",
    "\n",
    "|               | Signification |\n",
    "|---------------|----------------------------|\n",
    "| $Y$       | variable cible (target) |\n",
    "| $\\hat{f}$ | fonction estimée de $f$ |\n",
    "| $biais$  | erreur carrée moyenne |\n",
    "| $variance$    | quantité de variation |\n",
    "| $\\sigma$ | $sigma^2 = \\epsilon$ (bruit) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle Ridge \n",
    "Le modèle Ridge est une version pénalisée du modèle linéaire multiple dont la fonction de coût est :\n",
    "\n",
    "$$\n",
    "||y-X^{t}\\beta||^{2}_{2}+\\lambda||\\beta||^{2}_{2},\\lambda\\in\\mathbb{R^{+*}}\n",
    "$$ \n",
    "\n",
    "* $\\lambda = 0$ : Variance au Max (Overfitting)\n",
    "* $\\lambda$ augmentent : Biais augmentent & Variance diminue\n",
    "* $\\lambda = \\infty$ : Biais au Max (Underfitting)\n",
    "\n",
    "_Ridge réduit les coefficients associés aux variables, mais il fonctionne de manière globale._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle Lasso \n",
    "Le modèle LASSO (Least Absolute Shrinkage and Selection Operator) dont la fonction de coût est :\n",
    "\n",
    "$$\n",
    "|||Y-X^{t}\\beta||^{2}_{2}+\\lambda|||\\beta||_{1}\n",
    "$$\n",
    "\n",
    "* $\\lambda$ augmentent : Biais augmentent & Variance diminue\n",
    "* $\\lambda$ diminue : Le nombre de coefficients non nuls augmentera (Overfitting)\n",
    "\n",
    "_Le modèle Lasso filtre les variables explicative les plus inutiles._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting and Overfitting\n",
    "\n",
    "* __Underfitting :__ est le fait qu'un modèle est trop simple pour renvoyer une bonne estimation de la variable cible.\n",
    "\n",
    "* __Overfitting :__ lorsqu'un modèle correspond trop bien aux données d'entraînement.\n",
    "\n",
    "![](https://datascience.foundation/img/pdf_images/underfitting_and_overfitting_in_machine_learning_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross validation\n",
    "\n",
    "Cette cross validation augmente notre nombre de ligne ($n$) en multipliant le nombre d'observation($k$).\n",
    "\n",
    "![overfitting](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search cross validation\n",
    "\n",
    "Cette cross validation combine plusieurs modèles de machine learning avec differents parametre.\n",
    "\n",
    "Grille de parametres :\n",
    "```\n",
    "grid = { \n",
    "    'n_estimators': [200,300,400,500],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy'],\n",
    "    'random_state' : [18]\n",
    "}\n",
    "```\n",
    "et teste toute les combinaisons (ici $4\\times2\\times5\\times2\\times1 = 80$ modèles testés)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régressions logistique (Classification)\n",
    "\n",
    "\n",
    "$$\n",
    "P(Y=1)=f(X)+\\epsilon\n",
    "$$\n",
    "\n",
    "|     | Signification |\n",
    "|-----|----------------------------|\n",
    "| $P$ | Probabilité que |\n",
    "| $Y$ | la variable Cible (target) |\n",
    "| $f$ | fonction |\n",
    "| $X$ | la matrice Explicative |\n",
    "| $\\epsilon$  | le bruit (Episilon) |\n",
    "\n",
    "## fonction logistique \n",
    "\n",
    "$$\n",
    "f(X) = \\mathbb{sigmoid}(X) = \\frac{1}{1+\\mathbb{exp}(-X)}\n",
    "$$\n",
    "\n",
    "dans lequel : \n",
    "\n",
    "$$\n",
    "X = \\beta_{0}+X_{1}\\beta_{1}+...+X_{p}\\beta_{p}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fonction Logistique (Sigmoid) :__\n",
    "\n",
    "![logistic_function](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/logistic_function.png)\n",
    "\n",
    "Y indique si notre resultat est vrai (Y = 1) ou faux (Y = 0).\n",
    "\n",
    "La courbe bleue représente la probabilité en fonction de la valeur de X.\n",
    "\n",
    "Le tout dans l'intervalle [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonction de coût \n",
    "\n",
    "En Supervised Learning, la machine cherche les paramètres de modèle qui minimisent la Fonction Coût.\n",
    "\n",
    "Dans le cas de la régression logstique la fonction de coût (Log Loss):\n",
    "\n",
    "$$\n",
    "Log Loss = \\sum_{i=0}^n -y_ilog(\\hat{y_i}) - (1-y_i)log(1-\\hat{y_i})\n",
    "$$\n",
    "\n",
    "|     | Signification |\n",
    "|-----|----------------------------|\n",
    "| $y_i$ | la variable Cible (target) |\n",
    "| $\\hat{y_i}$ | prédiction de la variable Cible |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de confusion \n",
    "\n",
    "|Confusion Matrix| Model Pediction : __Positive__ | Model Pediction : __Negative__ ||\n",
    "|---|---|---|---|\n",
    "|__Reality : Positive__ |True Positive (TP) |False Negative (FN)|$$ {Sensitivity : } \\\\ \\frac{TP}{TP+FN}$$|\n",
    "|__Reality : Negative__ |False Positive (FP)|True Negative (TN) |$$ {Specificity : } \\\\ \\frac{TP}{TN+FP}$$|\n",
    "||$$ {Precision : } \\\\ \\frac{TP}{TP+FP}$$|$$ {Neg. Predictive Value : } \\\\ \\frac{TN}{TN+FN}$$|$$ {Accuracy : } \\\\ \\frac{TP+TN}{TN+FP+FP+FN}$$|\n",
    "\n",
    "Accuracy : représente la proportion de predictions exacte produite par le modèle.\n",
    "\n",
    "True Positive Rate (TPR) : Sensitivity or Recall : Taux Positif réel\n",
    "\n",
    "False Positive Rate (FPR) : Fallout : Taux de Faux Positif : $\\frac{FP}{FP+TN}$\n",
    "\n",
    "Precision : $\\frac{TP}{TP+FP}$\n",
    "\n",
    "### Score F1 :\n",
    "Le Score F1 est égale à la moyenne harmonique entre le rappel et la précision pour une classe donnée de la variable cible.\n",
    "$$\n",
    "\\frac{Precision + Recall}{Precision \\times Recall}\n",
    "$$\n",
    "\n",
    "### Courbes ROC et AUC :\n",
    "\n",
    "__Courbe ROC__\n",
    " * Pour visualiser la performance d'un modèle de classification binaire dans son ensemble.\n",
    " * Cette courbe est obtenue en traçant le taux TPR en fonction du taux FPR. \n",
    " * Idéalement cette courbe vient épouser le coin en haut à gauche.\n",
    " * Si celle ci s'approche de la droite, la prediction n'a aucun interêt.\n",
    "\n",
    " ![](https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png)\n",
    "\n",
    "__AUC (Area Under the Curve) :__\n",
    "\n",
    " L'ASC est interprétée comme la probabilité que le modèle donne un score plus élevé à une observation positive sélectionnée au hasard qu'à une observation négative sélectionnée au hasard.\n",
    "\n",
    " Un ASC < 50% signifie que le modèle ne prévoit pas mieux que le hazard complet.\n",
    "\n",
    " ### GINI coefficient : \n",
    "\n",
    " L'ASC est également liée à l'indice GINI, qui décrit la dispersion statistique de la population et est largement utilisé en économie pour quantifier les inégalités.\n",
    "\n",
    "$$\n",
    "GINI = 2AUC -1\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forest\n",
    "\n",
    "## Arbre de decision (regression and classification) (CART)\n",
    "\n",
    "L’arbre de décision est un algorithme itératif qui, à chaque itération, va séparer les individus en k groupes (généralement k=2 et on parle d’arbre binaire) pour expliquer la variable cible.\n",
    "\n",
    "La première division (on parle aussi de split) est obtenue en choisissant la variable explicative qui permet la meilleure séparation des individus. Cette division donne des sous-populations correspondant au premier nœud de l'arbre.\n",
    "\n",
    "Le processus de split est ensuite répété plusieurs fois pour chaque sous-population (noeuds précédemment calculés) jusqu'à ce que le processus de séparation s’arrête."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Critère de division :__\n",
    " * Recevabilité : si les branches resultante ont des noeuds non-vides.\n",
    " * Hétérogénéité : on essaye de minimiser la sommes des heterogeneité.\n",
    " \n",
    "Hétérogénéité avec $Y$ quantitatif :\n",
    "\n",
    "$$\n",
    "H_k=\\frac{1}{Card(k)}\\cdot\\sum_{i\\in{k}}(y_i-\\underline{y_k})^2\n",
    "$$\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|----------------------------|\n",
    "| $H$ | hétérogénéité |\n",
    "| $_k$ | nœud |\n",
    "| $Card(k)$ | nombre d'éléments dans le nœud k |\n",
    "| $\\cdot$ | $\\times$ |\n",
    "| $\\sum_{i\\in{k}}$ | sommes de l'ensemble des observations du nœud  |\n",
    "| $y_i$ | la variable Cible (target) |\n",
    "| $\\underline{y_k}$ | moyenne des valeurs de $Y$ parmi les observations du nœud $k$ |\n",
    "\n",
    "Nous essayons de trouver la division qui minimise la somme de la variance de Y dans la branche droite et gauche. Plus la variance de Y dans chaque branche est faible, plus les observations dans les deux branches sont homogènes vis-à-vis de la variable cible, si l'on construit un modèle capable d'isoler des groupes d'observations qui ont des valeurs similaires de Y dans la même branche, alors il y a de fortes chances que ce soit un bon prédicteur !\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Hétérogénéité avec $Y$ qualitatif (ou GINI):\n",
    "\n",
    "$$\n",
    "H_k=\\sum_{i=1}^{m}p_{k}^{i}\\cdot(1-p_{k}^{i})\n",
    "$$\n",
    "\n",
    "|           | Signification |\n",
    "|-----------|----------------------------|\n",
    "| $H$ | hétérogénéité |\n",
    "| $_k$ | nœud |\n",
    "| $m$ | nombre de modalités (catégories) |\n",
    "| $\\sum$ | somme |\n",
    "| $i=1$ | debut des iterations |\n",
    "| $p_{k}^{i}$ | proportion de la categorie $i$ de $Y$ dans le nœud $k$.|\n",
    "| $\\cdot$ | $\\times$ |\n",
    "\n",
    "Nous cherchons donc la division qui minimise la somme des hétérogénéités des nœuds enfants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Critère d'arrêt :__\n",
    " * homogène : lorsque toutes les observations dans le nœud ont la même valeur ou la même modalité de $Y$.\n",
    " * lorsqu'il n'y a plus de divisions recevables.\n",
    " * lorsq'il a atteint un cran d'arrêt prédéfini (```min_samples_split``` ou ```min_samples_leaf```).\n",
    " * lorsque nous atteignons une certaine profondeur (un certain nombre de subdivions répétées)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__lutter contre l' overfitting :__\n",
    "\n",
    "L'objectif est de trouver un arbre intermédiaire pour obtenir un compromis biais/variance en jouant avec les critères d'arrêt à l'aide de gridsearch :\n",
    "* ```min_samples_leaf```\n",
    "* ```min_samples_split```\n",
    "* ```max_depth```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "### __Bagging :__ contraction Bootstrap et Agrégation.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/512px-Ensemble_Bagging.svg.png)\n",
    "\n",
    "Étant donné un ensemble d'entraînement standard $D$ de taille $n$, le bagging génère $m$ nouveaux ensembles d'entrainement $D_i$, chacun de taille $n'$, par échantillonage uniforme. Il peut y avoir des doublons de variable observé d'un échantillon à l'autre. Appellé échantillon de Bootstrap. Puis, $m$ modèles sont entrainées à l'aide des $m$ échantillons de bootstrap. Pour finir, la prédiction du meta-modèle est obtenue en faisant la moyenne de la sortie (pour la régression) ou par vote de majorité (pour la classification) des $m$ modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Y quantitatif :__ le modèle agrégé est la moyenne des fonctions estimées par les modèles, la moyenne des valeurs de $Y$ pour une observation donnée.\n",
    "\n",
    "$$\n",
    "\\hat{f}_B(.)=\\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}_{z_b}(.)\n",
    "$$\n",
    "\n",
    "__Y qualitatif :__ le modèle agrégé est le vote majoritaire parmi les fonctions estimées par les modèles, la modalité de $Y$ les plus représentés parmi les réponses des différents modèles à une observation donnée.\n",
    "\n",
    "$$\n",
    "\\hat{f}_B(.)=arg\\max_{i}Card\\{b|\\hat{f}_B(.)=j\\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "__Mean decrease Accuracy :__ Cette méthode consiste à échanger aléatoirement les valeurs d'une variable explicative. La différence entre l'erreur de validation avant et après le changement est ensuite mesurée. Plus l'erreur de validation pré-commutation est élevée, plus la variable en question est considérée comme importante pour la prédiction de la variable cible.\n",
    "\n",
    "__Mean decrease Gini :__ Cette méthode permet d'évaluer l'importance d'une variable au niveau d'un nœud : elle mesure la diminution de la fonction d'hétérogénéité si nous réutilisons la variable explicative utilisée pour le nœud par celle que nous voulons évaluer. L'importance générale de la variable est alors une somme des diminutions de l'hétérogénéité mesurées et pondérées par le nombre d'observations à chaque nœud.\n",
    "\n",
    "__Controle du bais et de la variance :__ Les arbres de décision cessent naturellement de diviser le jeu de données lorsque toutes les branches contiennent des sous- ensembles de données dans lesquels la variable cible est homogène (la même valeur pour toutes les observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "Les SVM sont des modèles adaptés aux problèmes de classification. Ils cherchent à construire la limite la plus éloignée possible entre les observations appartenant aux différentes classes.\n",
    "\n",
    "__Linearl kernel :__\n",
    "\n",
    "L'idée sous-jacente est de séparer les points appartenant à différentes classes en trouvant un hyperplan. \n",
    "\n",
    "les observations sont représentées sous la forme d'un nuage de points dans un $d$ espace dimensionnel où $d$ est le nombre de variables explicatives que nous utilisons pour la modélisation.\n",
    "\n",
    "Nous calculons la distance entre les vecteurs de support et la ligne de séparation, cette distance est appelée la marge. Le but de Support Vector Machines est de maximiser la marge, comme le montre l'exemple ci-dessous :\n",
    "\n",
    "![](https://www.researchgate.net/profile/Seyyid-Ahmed-Medjahed/publication/282464007/figure/fig1/AS:296526237716481@1447708772146/The-optimal-Hyperplane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear SVM (Kernel-based estimators)\n",
    "\n",
    "__Polynomial kernel :__\n",
    " \n",
    "nous pouvons produire une nouvelle dimension grâce à un grain polynomial $k$, qui est la fonction du noyau polynomial. Cette nouvelle dimension n'est pas une combinaison linéaire des variables explicatives et augmente donc la dimension de l'espace d'observation de deux à trois.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Alexander-Smola/publication/229025155/figure/fig4/AS:300828989640727@1448734628993/Example-of-an-SV-classifier-found-using-a-radial-basis-function-kernel-k-x-x-exp.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Noyau radial :__\n",
    "\n",
    "Dans le cas d'un groupe entouré d'un second, ce problème peut être transformé en un problème linéairement séparable dans un espace plus vaste. Si nous ajoutons une troisième dimension $z = x^2 + y^2$ qui représente la distance entre un point et l'origine du système de coordonnées orthonormal\n",
    "\n",
    "![](https://i.stack.imgur.com/qyU9m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__lutter contre l' overfitting :__\n",
    "\n",
    "Pour les modèles SVM, comme pour tout modèle avec une variance naturellement élevée et un biais faible, il est essentiel de trouver un bon compromis de variance de biais afin d'éviter un ajustement excessif :\n",
    "* C penalty : \n",
    "    * Pour de grandes valeurs de C, l'optimisation choisira un hyperplan à plus petite marge si cet hyperplan fait un meilleur travail pour classer correctement tous les points d'entraînement. \n",
    "    * Pour de petite valeur de C amènera l'optimiseur à rechercher un hyperplan séparant une plus grande marge, même si cet hyperplan classe à tort plus de points.\n",
    "* Gamma parameter : \n",
    "    * Le paramètre gamma est une mesure de la portée de l'influence de chaque observation lors de la construction de la frontière de décision, des valeurs élevées de gamma se traduisent par des zones d'influence très petites, lorsque le gamma est faible (proche de zéro), la zone d'influence de chaque observation est très grande.\n",
    "    * Gamma : 0 = Overfitting  variance élevé.\n",
    "    * Gamma : $\\infty$ =  Underfitting biais élevé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting - ADABOOST\n",
    "\n",
    "Le boosting permet d'améliorer les performances d'un modèle simple d'apprentissage automatique supervisé en répétant l'étape d'apprentissage plusieurs fois de suite tout en tenant compte des erreurs de prédiction, puis en combinant les différents modèles. Cette méthode utilise la notion d'agrégation.\n",
    "\n",
    "La méthode AdaBoost construit  des modèles successivement amélioré en modifiant les poids attribués aux observations. Elle augmente l'importance de celles qui causent des erreurs et minimise l'importance de celles qui sont correctement classées par le modèle. Au final, les modèles sont agrégés, et le poids de chaque modèle est retenue en fonction de leur erreur d'apprentissage.\n",
    "\n",
    "__le modèle final obtenu à partir de l'algorithme AdaBoost :__\n",
    "\n",
    "\n",
    "$$H(X)=sign(\\sum_{t=1}^{T}\\alpha_t{h_t}(X))=sign(F(x))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditions de performance \n",
    "\n",
    "* Il doit avoir été formé sur un nombre suffisant d'exemples\n",
    "* Il doit être capable de décrire avec précision les observations de la base de formation.\n",
    "* Il doit être simple\n",
    "\n",
    "__Deuxième critère de performance__\n",
    "* communément appelé condition d'apprentissage faible, correspond à l'idée que chaque modèle de classification binaire qui constitue le modèle final a une erreur d'entraînement qui est plus petite que le pur hasard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting AdaBoost \n",
    "\n",
    "Une fois ces deux conditions vérifiées, l'algorithme AdaBoost réduit l'erreur d'apprentissage à un niveau minimum en relativement peu d'itérations.\n",
    "\n",
    "Cependant, comme c'est souvent le cas en statistique, une erreur minimale d'entraînement signifie souvent un surajustement\n",
    "\n",
    "__Marges__ \n",
    "\n",
    "Il existe un cadre théorique dans lequel l'algorithme AdaBoost fournit un modèle où l'erreur sur l'échantillon de validation atteint un niveau optimal, et où le modèle produit s'améliore au fur et à mesure des itérations.\n",
    "\n",
    "La marge réagit comme un système de vote qui fait davantage confiance à une décision majoritaire.\n",
    "\n",
    "La marge est une valeur qui fluctue dans la plage [-1.1] qui est définie comme suit:\n",
    "\n",
    "$$\n",
    "Y_{i}F(x)=\\sum_{t:Y_i=h_t(X_i)}\\alpha_t\\;-\\sum_{t:Y_i\\neq{h_t}(X_i)}\\alpha_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting (XGBoost)\n",
    "\n",
    "XGBoost (ou contraction de eXtreme Gradient Boosting) est un modèle de machine Learning basé sur l'apprentissage d'ensemble séquentiel et les arbres de décision. Comme son nom l'indique, il utilise le boosting de gradient.\n",
    "\n",
    "Considérons un classifieur faible initial $f0$. Après l'avoir optimisé, la méthode de boosting cherche à construire un nouveau classifieur faible $f1$ à partir de $f0$ en introduction un terme de résidu $h$ :\n",
    "$$\n",
    "f_1(x) = f_0(x) + h(x)\n",
    "$$\n",
    "\n",
    "De sorte que $f1$ soit plus performant que $f0$. On répétant l'opération un certain nombre de fois, disons $p$, on construit un classifieur final $F$ complexe qui est une combinaison linéaire des $f_i$, où chacun des \n",
    "$f_i$ est associé à un poids $\\alpha i$ :\n",
    "\n",
    "$$\n",
    "F(x) = \\sum^n_{i=1} \\alpha _i f_i(x)\n",
    "$$\n",
    "\n",
    "Le Gradient Boosting est une technique particulièrement puissante dans le cas où la fonction de perte est différentiable.\n",
    "\n",
    "L'apprentissage d'un XGBoost est plus long que les autres modèles de ML mais est considéré comme le meilleur de sa catégorie pour les données structurées/tabulaires de petite à moyenne taille.\n",
    "\n",
    "[Info](https://blent.ai/xgboost-tout-comprendre/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "## Voting / Averaging\n",
    "\n",
    "Nous parlons de \"Voting classifier\" lorsque la variable cible est qualitative.\n",
    "nous formerons plusieurs modèles différents (par exemple, un Bayes naïf, un SVM et une forêt aléatoire). Chacun des modèles donnera une certaine prédiction $\\hat{y}_1$, $\\hat{y}_2$, $\\hat{y}_3$ et un certain score  $L1$, $L2$, $L3$. Le classificateur de vote est un modèle agrégé dont les prédictions sont calculées par un vote pondéré des différents modèles.\n",
    "\n",
    "Qui peut être écrit :\n",
    "\n",
    "$$\n",
    "\\hat{y} = {argmax}_{m \\epsilon M} ( S_1 \\times (\\hat{y}_1 = m) +  S_2 \\times (\\hat{y}_2 = m) +  S_3 \\times (\\hat{y}_3 = m) )\n",
    "$$\n",
    "\n",
    "|             | Signification |\n",
    "|-------------|----------------------------|\n",
    "| $\\hat{y}$ | prédiction  |\n",
    "| $m$ | modalité (catégories) |\n",
    "| $M$ | Ensemble des catégories |\n",
    "| $S$ | Score |\n",
    "\n",
    "Chacun de ces modèles donne une prédiction pour $y$ : $\\hat{y}_1$, $\\hat{y}_2$, $\\hat{y}_3$ et un certain score : $S1$, $S2$, $S3$. La prédiction finale sera la moyenne pondérée en fonction du score des prédictions des modèles utilisés.\n",
    "\n",
    "Qui peut être écrit :\n",
    "\n",
    "$$\n",
    "\\hat{y}=\\frac{1}{S_1+S_2+S_3}\\times(S_1\\times\\hat{y}_1+S_2\\times\\hat{y}_2+S_3)\n",
    "$$\n",
    "\n",
    "|             | Signification |\n",
    "|-------------|----------------------------|\n",
    "| $\\hat{y}$ | prédiction  |\n",
    "| $S$ | Score |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Le stacking (ou dit parfois blending) est un procédé qui consiste à appliquer un algorithme de machine learning à des classifieur générés par un autre algorithme de machine learning.\n",
    "\n",
    "D’une certaine façon, il s’agit de prédire quels sont les meilleurs classifieurs et de les pondérer. Cette démarche a l’avantage de pouvoir agréger des modèles très différents et d’améliorer sensiblement la qualité de la prédiction finale.\n",
    "\n",
    "Pour pouvoir évaluer les différents modèles à stacker, on utilise des poids constants permettant de pondérer les différents modèles. C’est du stacking linéaire standard.\n",
    "\n",
    "La principale limitation est qu’on a perdu la dépendance des modèles à leur data et plus particulièrement leur spécialisation. On va donc utiliser des poids variables qui dépendent des données ; soit des méta-features. Le stacking est cette fois-ci dépendant du poids des features.\n",
    "\n",
    "[info](https://blog.octo.com/les-methodes-ensemblistes-pour-algorithmes-de-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
